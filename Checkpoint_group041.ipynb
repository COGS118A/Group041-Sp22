{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Will Sumerfield\n",
    "- Miguel Monares\n",
    "- Abdalla Atalla\n",
    "- Ritik Raina\n",
    "- Matilda Michel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "The goal of our project is to learn about the strengths, weaknesses, and applications of several popular supervised\n",
    "machine learning models. We will hypothesize their performance predicting Heart Disease on a large dataset of health\n",
    "related metrics, train each model on the same data, and analyze their performance. Finally, we will attempt to create\n",
    "an ensemble model which outperforms each of our individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Heart disease is the leading cause of death in the United States, accounting for more than 696,000 deaths in 2020 alone. [1]. It is a disease that, if remedied proactively early, can be treated and/or alleviated. Hence, in order to best address the prominence of heart disease in the US, we must use leading technologies to help understand, predict, and detect indicators of heart disease in patients.\n",
    "\n",
    "The technology of machine learning is already widely used in the field of medicine, including the domain of disease prediction & classification.\n",
    "\n",
    "### Prior Work:\n",
    "\n",
    "1. Using patient information such as MRI scans, biomarkers, and numerical data about the patient, researchers have been able to develop a random forest classifier that predicts Alzheimer's Disease in its early stages up to an 85% accuracy [2]. The implications of this research are proactive care for those who are predicted to develop Alzheimer's disease.\n",
    "2. Using feature selection and data cleaning techniques, researchers have been able to leverage to use of different machine learning models on multiple sources of data (meterological, epidemic, media data, etc.) to analyze, predict, and prevent the spread of infectious diseases [3].\n",
    "3. Finally, within the space of heart disease classification, researchers have already developed machine learning and deep learning models to predict heart disease. Using a Heart Disease dataset from UCI, researchers have been able to leverage deep learning techniques to achieve a 94% accuracy in predicting heart disease [4].\n",
    "\n",
    "As demonstrated in the previous research that has been done in this field, many different models and data science techniques have been employed in the objective of disease classification. It's important that we understand how we can use the leading technology of machine learning to best predict and classify health conditions, like heart disease, in order to promote healthy habits and preventative measure for those who are at risk. This facet of understanding motivates the purpose of our project.\n",
    "\n",
    "[1] https://www.cdc.gov/nchs/fastats/leading-causes-of-death.htm\n",
    "[2] https://www.frontiersin.org/articles/10.3389/fpubh.2022.853294/full#:~:text=The%20Machine%20Learning%20techniques%20(26,algorithms%20(28%2C%2029).\n",
    "[3] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8219638/\n",
    "[4] https://www.hindawi.com/journals/cin/2021/8387680/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "In this project, we are trying to find the strengths, weaknesses, and applications of the following supervised\n",
    "machine learning models: Gaussian Process Classification, Support Vector Classification, Decision Trees,\n",
    "K-Nearest Neighbors, MLP Classification, and Polynomial Classification.\n",
    "\n",
    "Each of us has picked one of the aforementioned supervised machine learning models. With that model, each of us will\n",
    "make a hypothesis about the model's performance on our dataset, train the model, and then analyze the model's\n",
    "performance on the dataset.\n",
    "\n",
    "To test our models, we are using a Dataset containing health-related features for over 300,000 subjects, and a column\n",
    "specifying whether that subject has Heart Disease or not. Our models will be tasked with predicting Heart Disease given\n",
    "the other features, the performance of which will be used to measure the accuracy of our hypotheses.\n",
    "\n",
    "Before we work with the data, we will perform EDA to look for oddities and important information in our dataset.\n",
    "Although we expect that all features in our dataset are somewhat relevant, there is a chance that some features\n",
    "are not worth including the dataset. We will use a mixture of automated feature selection and common sense to choose\n",
    "which (if any) features are excluded. Additionally, we may choose to perform feature extraction on the data, and create\n",
    "more features.\n",
    "\n",
    "In order to hypothesize the performance of a model, we will be taking into account the size of the dataset, the\n",
    "difficulty of the prediction task (Predicting Heart Disease on a dataset of various health related metrics for\n",
    "subjects), the number of and types of features, and the shape/distribution of the data.\n",
    "\n",
    "Then, we will train our model on the data. Each of us will use the same preprocessed data before giving them to\n",
    "our models, so that the models are easier to compare, although some models may use additional preprocessing on the data.\n",
    "\n",
    "Next, we will measure the performance of the model using F1, Precision, and Accuracy scores on the training and testing'\n",
    "datasets. Using that knowledge, we will attempt to figure out how and why our model's performance deviated from our\n",
    "expectations.\n",
    "\n",
    "Finally, we will attempt to merge our models into an ensemble, and see if we can best the performance of each of our\n",
    "individual models using this method.\n",
    "\n",
    "## Hypotheses:\n",
    "\n",
    "### Gaussian Process Classification\n",
    "*By Will Sumerfield*\n",
    "\n",
    "I predict that the Gaussian Process Classifier model will perform very well, if I can find a good kernel function for\n",
    "the data. However, I expect that this will be a very difficult process, given that there are many columns of our data.\n",
    "Additionally, I may need to use smaller subsamples of the data to train the GPC, given that GPCs take up **$$O(n^2)$$**\n",
    "space, and take the same training time. I expect this model to be among, if not the best model we employ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "[Dataset Link](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from matplotlib.patches import Patch\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data General Knowledge\n",
    "\n",
    "As we can see below, our dataset is a collection of information related to heart health. The first column is a *True*\n",
    "or *False* value which tells us whether that row's person has some form of heart disease. This is the variable we will\n",
    "be trying to predict with our models, based on the information the other features provide.\n",
    "\n",
    "We can see below that we have 17 different features with which to predict heart disease, and over 300,000 data points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the data as a dataframe\n",
    "data = pd.read_csv(\"data/raw_data.csv\")\n",
    "\n",
    "# Display the number of columns in the dataframe\n",
    "print(f\"Number of Raw Features: {len(data.columns)}\")\n",
    "print()\n",
    "print(f\"Number of Datapoints: {data.shape[0]}\")\n",
    "print()\n",
    "\n",
    "# Display the head of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Visualization\n",
    "\n",
    "In order to see what our data looks like, and correlations between the features and heart disease, we can plot or\n",
    "display each feature against Heart Disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Heart Disease\n",
    "\n",
    "It is good idea to start by looking at the proportion of heart disease among the subjects.\n",
    "As we can see below, heart disease is relatively rare. This means that we will likely need to use data sampling methods\n",
    "which equivocate the number of samples with and without Heart Disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get a subset of the data for graphing\n",
    "data = data.sample(100000)\n",
    "\n",
    "# Get the heart disease data\n",
    "heart_disease = data[\"HeartDisease\"]\n",
    "\n",
    "# Get an axis to plot to\n",
    "fig, axis = plt.subplots(figsize=(10, 8), dpi=80)\n",
    "\n",
    "# Plot the heart disease proportions\n",
    "sns.barplot(x=heart_disease.unique(), y=heart_disease.value_counts()/heart_disease.value_counts().sum(),\n",
    "           ax=axis, linewidth=2, edgecolor='0.3')\n",
    "\n",
    "# Set the plot aesthetics\n",
    "axis.set_title(\"Heart Disease Proportions\", fontsize=30)\n",
    "axis.set_yticks([round(0.1 * i, 1) for i in range(11)])\n",
    "axis.set_yticklabels(axis.get_yticks(), size=10)\n",
    "axis.set_xticklabels(labels=axis.get_xticklabels(), size=10)\n",
    "axis.set_ylabel(\"Heart Disease Proportion\", fontsize=20)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Continuously Valued Features\n",
    "\n",
    "Some of the features in the dataset have continous values. In this case, it is helpful to plot them as a boxplot,\n",
    "so that we can see the spread of the data. Below, we can see that Physical and Mental Health are strongly right skewed,\n",
    "whereas BMI and Sleep Time are more normal in shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get all continuously valued features\n",
    "continuous_features = ['BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime']\n",
    "\n",
    "# Get an axis to plot to\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8), dpi=80)\n",
    "\n",
    "# Add spacing between plots\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Create a boxplot for each feature\n",
    "for axis, feature in zip(axes.flatten(), continuous_features):\n",
    "\n",
    "    # Plot the feature\n",
    "    sns.boxplot(x=feature, data=data, ax=axis)\n",
    "\n",
    "    # Set the plot aesthetics\n",
    "    axis.set_title(feature, fontsize=20)\n",
    "    axis.set_xlabel('')\n",
    "\n",
    "\n",
    "# Display all box plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Comparison to Heart Disease\n",
    "\n",
    "Now that we understand the spread of each of the continuous features, we should try to see how they\n",
    "correlate with Heart Disease.\n",
    "\n",
    "Below, we can clearly see that, predictably, people's reported Physical Health appears to correlate strongly with\n",
    "Heart Disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a subplot for each feature in the dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), dpi=80)\n",
    "\n",
    "# Adjust the spacing between plots\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Graph each continuous feature\n",
    "for index in range(len(continuous_features)):\n",
    "\n",
    "    # Find the axis and feature\n",
    "    axis = axes.flatten()[index]\n",
    "    feature = continuous_features[index]\n",
    "\n",
    "    # Plot the feature against heart disease\n",
    "    sns.boxplot(x=heart_disease, y=feature, data=data, ax=axis)\n",
    "\n",
    "    # Set the plot aesthetics\n",
    "    axis.set_title(feature, fontsize=20)\n",
    "    axis.set_yticklabels(axis.get_yticks(), size=10)\n",
    "    axis.set_xticklabels(['False', 'True'], size=10)\n",
    "    axis.set_xlabel(\"Heart Disease\", fontsize=12)\n",
    "    axis.set_ylabel(\"\")\n",
    "\n",
    "# Display all the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Nominal Features\n",
    "\n",
    "Now, we should analyze the trends of the nominal features. These values are mostly boolean, with some exceptions.\n",
    "We can make use of stacked bar-plots to vizualize both the proportions within each feature, as well as the proportions of\n",
    "heart disease for each value of that feature.\n",
    "\n",
    "Below, we can see that many of our features do not have even distributions across their values.\n",
    "This tells us that we will need to up/down sample our data in order to compensate. N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get all nominal features\n",
    "nominal_features = ['Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex', 'Diabetic', 'PhysicalActivity',\n",
    "                    'Asthma', 'KidneyDisease', 'SkinCancer', 'Race', 'AgeCategory', 'GenHealth']\n",
    "\n",
    "# Set the order of each nominal feature's values\n",
    "nominal_feature_orders = [\n",
    "    ['No', 'Yes'],\n",
    "    ['No', 'Yes'],\n",
    "    ['No', 'Yes'],\n",
    "    ['No', 'Yes'],\n",
    "    ['Female', 'Male'],\n",
    "    ['No', 'Yes', 'No, borderline diabetes', 'Yes (during pregnancy)'],\n",
    "    ['No', 'Yes'],\n",
    "    ['No', 'Yes'],\n",
    "    ['No', 'Yes'],\n",
    "    ['No', 'Yes'],\n",
    "    ['White', 'Black', 'Hispanic', 'Asian', 'American Indian/Alaskan Native', 'Other'],\n",
    "    sorted(data['AgeCategory'].unique(), key=lambda x: int(re.match('\\d{2}?', x).group())),\n",
    "    ['Poor', 'Fair', 'Good', 'Very good', 'Excellent']\n",
    "]\n",
    "\n",
    "# Create a subplot for each feature in the dataset\n",
    "fig, axes = plt.subplots(len(nominal_features), figsize=(10, 120), dpi=80)\n",
    "\n",
    "# Adjust the spacing between plots\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Graph each nominal feature\n",
    "for index in range(len(nominal_features)):\n",
    "\n",
    "    # Find the axis and feature\n",
    "    axis = axes[index]\n",
    "    feature = nominal_features[index]\n",
    "\n",
    "    # Find the feature dataframe\n",
    "    feature_data = data[feature]\n",
    "\n",
    "    # Find the number of heart disease cases with the feature\n",
    "    heart_disease_false = [data[(feature_data == f_class) & (heart_disease == 'No')].shape[0] / feature_data.shape[0]\n",
    "                           for f_class in nominal_feature_orders[index]]\n",
    "    heart_disease_true = [data[(feature_data == f_class) & (heart_disease == 'Yes')].shape[0] / feature_data.shape[0]\n",
    "                           for f_class in nominal_feature_orders[index]]\n",
    "    for f_class in range(len(heart_disease_true)): heart_disease_true[f_class] += heart_disease_false[f_class]\n",
    "\n",
    "    # Plot the feature against heart disease\n",
    "    sns.barplot(x=nominal_feature_orders[index], y=heart_disease_true, ax=axis, color=sns.color_palette()[2],\n",
    "                linewidth=2, edgecolor='0.3')\n",
    "    sns.barplot(x=nominal_feature_orders[index], y=heart_disease_false, estimator=sum, ci=None, ax=axis,\n",
    "                color=sns.color_palette()[3], linewidth=2, edgecolor='0.3')\n",
    "\n",
    "    # Set the plot asesthetics\n",
    "    axis.set_title(feature, fontsize=30)\n",
    "    axis.set_yticks([round(0.1 * i, 1) for i in range(11)])\n",
    "    axis.set_yticklabels(axis.get_yticks(), size=10)\n",
    "    axis.set_xticklabels(labels=axis.get_xticklabels(), size=10)\n",
    "    axis.set_ylabel(\"Feature Proportion\", fontsize=20)\n",
    "    axis.legend(handles=[Patch(facecolor=sns.color_palette()[2]), Patch(facecolor=sns.color_palette()[3])],\n",
    "                title=\"Heart Disease\", labels=['Yes', 'No'], title_fontsize=20, fontsize=15)\n",
    "\n",
    "# Display all the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "\n",
    "We will use 3 different metrics to assess the performance of our models on our task of Heart Disease Prediction/Classification.\n",
    "\n",
    "The first metric we will use is Recall/Sensitivity (formula shown below). We will use this metric to assess performance of our task because recall measures the model's ability to classify the true positives in its predictions. This is relevant to the problem we are solving because we want our model to miss as few truly heart disease-prone individuals as possible. It is very expensive to miss a true positive (miss a heart disease prone patient) in this context.\n",
    "\n",
    "The second metric we will consider is Precision (formula shown below). We will use this metric to measure the performance of our model because it tells us how many predicted positives are actually positive. In the context of our problem, this metric answers the question: How many patients predicted to have heart disease actually had heart disease? Recall is a measure that can benefit from biasing predictions toward all true, so we use Precision to ensure that our model is still making balanced and accurate predictions.\n",
    "\n",
    "![Precision and Recall](assets/PrecisionRecall_formula.png)\n",
    "\n",
    "The last metric we will consider is F1 as it is the harmonic balance between Precision and Recall. We use F1 to make sure that our models predictions are measuring Prediction and Recall in a balanced manner, so that we assess if our model is biased towards predicting heart disease or non heart disease.\n",
    "\n",
    "![F1](assets/F1_formula.png)\n",
    "\n",
    "In the evaluation of our models, we will primarily be looking at Recall/Sensitivity, as it is the most relevant metric to the problem we are trying to address, which is the prediction/classification of heart disease. However, we will take the other two metrics into account as they provide information into the general accuracy of our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When developing machine learning applications, its important to be aware and proactive against ethics and privacy concerns. The dataset we use does not include Personal Identifiable Information (PII), which helps ensure that the privacy of patients and kept.\n",
    "\n",
    "Ethical concerns can arise in the results processing and analysis stage. Its important that the results are validated to make sure they are reasonable, so that possible correlations in the underlying data are pursued and percieved to be truth. For example, if a correlation between race and diabeties is found, its important to not jump to conclusions and declare causality/truth behind the correlation.\n",
    "\n",
    "Additionally, we need to make sure that the data we use to train our model is representative of the general population, so that we can best fit our model to classify heart disease in patients with different types of backgrounds .\n",
    "\n",
    "In the field of medicine, its important that our product is tested and verified in the interest of liability. While it is important that we miss as few positively predicted heart disease patients in our model, we need to make sure that our model doesn't overly classify non heart disease prone patients as positive as this would lead to resources and facilities being directed toward those who don't need it.\n",
    "\n",
    "In the field, it is possible that this product may produce some unintended consequences relating to ethics or privacy. If this product is release into the field, we will make sure to address any unintended outcomes breaching ethical or privacy concerns by working to understand why such breaches occurred and being proactive about alleviating such conerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "* *Each of us expect to learn about the dataset and the features it entails.*\n",
    "* *Each of us should pick a particular ML model to evaluate our dataset on and write our own code.*\n",
    "* *In the case that any of our teammates reach issues concerning ML results; we can do additional meetings to help out.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time | Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/1  |  4 PM | Peer reviews + model research, data visualization + cleaning | Discuss peer reviews + any ideas we’ve gotten from them, discuss + compare ML models we researched to start deciding what we want to use |\n",
    "| 5/8  |  4 PM |  Peer reviews + indv. code work on feature selection + exploring our models | Further discuss models we want to use, decide what features we want to use to make prediction|\n",
    "| 5/15  | 4 PM  | more exploring our models + feature selection | Finish data cleaning + most of EDA, discussed having issues w/ usable features --> decided to switch to another dataset + classfication task|\n",
    "| 5/22  | 4 PM  | code work on indv. models + finish/turn-in checkpoint + EDA| have EDA + visualization finished, check-in with indv. model exploring + do any coding we want to do together, discuss current state of our tasks, see if theres anything to help each other with/do together |\n",
    "| 5/29  | 4 PM  | have most indv. project/model coding done | Go over + clean up coding aspect of project + compare performance of models, start report/assign sections to do (focus on intro/hypoth/methods/background) |\n",
    "| 6/5  | 4 PM  | Indv. report work + maybe any final touches to coding | Another coding + report draft check in/discussion, assign any finishing touches on either |\n",
    "| 6/8  | 4 PM  | Finishing touches | Final check-in + reviewing project --> TURN IN FINAL PROJECT |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Group041-Sp22)",
   "language": "python",
   "name": "pycharm-b7952dfa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
