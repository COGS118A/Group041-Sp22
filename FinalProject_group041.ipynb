{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Heart Disease Model Search\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Will Sumerfield\n",
    "- Miguel Monares\n",
    "- Abdalla Atalla\n",
    "- Ritik Raina\n",
    "- Matilda Michel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Important Files\n",
    "\n",
    "- common/model_training\n",
    "*The template code we used to train each of our models individually*\n",
    "- common/test_model_training\n",
    "*A data playground we used to test and learn about our models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Abstract \n",
    "\n",
    "The goal of our project is to learn about the strengths, weaknesses, and applications of several popular supervised\n",
    "machine learning models. We will hypothesize their performance predicting Heart Disease on a large dataset of health\n",
    "related metrics, train each model on the same data, and analyze their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "Heart disease is the leading cause of death in the United States, accounting for more than 696,000 deaths in 2020 alone.\n",
    "[1]. It is a disease that, if remedied proactively early, can be treated and/or alleviated. Hence, in order to best\n",
    "address the prominence of heart disease in the US, we must use leading technologies to help understand, predict, and\n",
    "detect indicators of heart disease in patients. The technology of machine learning is already widely used in the field\n",
    "of medicine, including the domain of disease prediction & classification.\n",
    "\n",
    "#### Prior Work:\n",
    "\n",
    "1. Using patient information such as MRI scans, biomarkers, and numerical data about the patient, researchers have\n",
    "been able to develop a random forest classifier that predicts Alzheimer's Disease in its early stages up to an 85%\n",
    "accuracy [2]. The implications of this research are proactive care for those who are predicted to develop Alzheimer's\n",
    "disease.\n",
    "2. Using feature selection and data cleaning techniques, researchers have been able to leverage to use of different\n",
    "machine learning models on multiple sources of data (meterological, epidemic, media data, etc.) to analyze, predict,\n",
    "and prevent the spread of infectious diseases [3].\n",
    "3. Finally, within the space of heart disease classification, researchers have already developed machine learning\n",
    "and deep learning models to predict heart disease. Using a Heart Disease dataset from UCI, researchers have been able\n",
    "to leverage deep learning techniques to achieve a 94% accuracy in predicting heart disease [4].\n",
    "\n",
    "As demonstrated in the previous research that has been done in this field, many different models and data science\n",
    "techniques have been employed in the objective of disease classification. It's important that we understand how we can\n",
    "use the leading technology of machine learning to best predict and classify health conditions, like heart disease,\n",
    "in order to promote healthy habits and preventative measure for those who are at risk. This facet of understanding\n",
    "motivates the purpose of our project.\n",
    "\n",
    "[1] [Leading Causes of Death](https://www.cdc.gov/nchs/fastats/leading-causes-of-death.htm)\n",
    "[2] [AlzAlzheimer's Prediction using ML](https://www.frontiersin.org/articles/10.3389/fpubh.2022.853294/full)\n",
    "[3] [Using ML to Limit Disease Spread](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8219638/)\n",
    "[4] [Predicting Heart Disease using ML](https://www.hindawi.com/journals/cin/2021/8387680/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "In this project, we are trying to find the strengths, weaknesses, and applications of the following supervised machine learning models: Gaussian Process Classification, Support Vector Classification, Decision Trees, K-Nearest Neighbors, MLP Classification, and Polynomial Classification.\n",
    "Each of us has picked one of the aforementioned supervised machine learning models. With that model, each of us will make a hypothesis about the model's performance on our dataset, train the model, and then analyze the model's performance on the dataset.\n",
    "To test our models, we are using a Dataset containing health-related features for over 300,000 subjects, and a column specifying whether that subject has Heart Disease or not. Our models will be tasked with predicting Heart Disease given the other features, the performance of which will be used to measure the accuracy of our hypotheses.\n",
    "Before we work with the data, we will perform EDA to look for oddities and important information in our dataset. Although we expect that all features in our dataset are somewhat relevant, there is a chance that some features are not worth including the dataset. We will use a mixture of automated feature selection and common sense to choose which (if any) features are excluded. Additionally, we may choose to perform feature extraction on the data, and create more features.\n",
    "In order to hypothesize the performance of a model, we will be taking into account the size of the dataset, the difficulty of the prediction task (Predicting Heart Disease on a dataset of various health related metrics for subjects), the number of and types of features, and the shape/distribution of the data.\n",
    "Then, we will train our model on the data. Each of us will use the same preprocessed data before giving them to our models, so that the models are easier to compare, although some models may use additional preprocessing on the data.\n",
    "Next, we will measure the performance of the model using F1, Precision, and Accuracy scores on the training and testing' datasets. Using that knowledge, we will attempt to figure out how and why our model's performance deviated from our expectations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "[Dataset Link](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib.patches import Patch\n",
    "import re\n",
    "pd.options.display.max_columns = 999\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data General Knowledge\n",
    "\n",
    "As we can see below, our dataset is a collection of information related to heart health. The first column is a *True*\n",
    "or *False* value which tells us whether that row's person has some form of heart disease. This is the variable we will\n",
    "be trying to predict with our models, based on the information the other features provide.\n",
    "\n",
    "We can see below that we have 17 different features with which to predict heart disease, and over 300,000 data points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Raw Features: 35\n",
      "\n",
      "Number of Datapoints: 319795\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Smoking_Yes</th>\n",
       "      <th>Smoking_No</th>\n",
       "      <th>AlcoholDrinking_No</th>\n",
       "      <th>AlcoholDrinking_Yes</th>\n",
       "      <th>Stroke_No</th>\n",
       "      <th>Stroke_Yes</th>\n",
       "      <th>DiffWalking_No</th>\n",
       "      <th>DiffWalking_Yes</th>\n",
       "      <th>Sex_Female</th>\n",
       "      <th>Sex_Male</th>\n",
       "      <th>Race_White</th>\n",
       "      <th>Race_Black</th>\n",
       "      <th>Race_Asian</th>\n",
       "      <th>Race_American Indian/Alaskan Native</th>\n",
       "      <th>Race_Other</th>\n",
       "      <th>Race_Hispanic</th>\n",
       "      <th>Diabetic_Yes</th>\n",
       "      <th>Diabetic_No</th>\n",
       "      <th>Diabetic_No, borderline diabetes</th>\n",
       "      <th>Diabetic_Yes (during pregnancy)</th>\n",
       "      <th>PhysicalActivity_Yes</th>\n",
       "      <th>PhysicalActivity_No</th>\n",
       "      <th>Asthma_Yes</th>\n",
       "      <th>Asthma_No</th>\n",
       "      <th>KidneyDisease_No</th>\n",
       "      <th>KidneyDisease_Yes</th>\n",
       "      <th>SkinCancer_Yes</th>\n",
       "      <th>SkinCancer_No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.58</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>23.71</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease    BMI  PhysicalHealth  MentalHealth  AgeCategory  GenHealth  \\\n",
       "0             0  16.60             3.0          30.0            7          4   \n",
       "1             0  20.34             0.0           0.0           12          4   \n",
       "2             0  26.58            20.0          30.0            9          1   \n",
       "3             0  24.21             0.0           0.0           11          2   \n",
       "4             0  23.71            28.0           0.0            4          4   \n",
       "\n",
       "   SleepTime  Smoking_Yes  Smoking_No  AlcoholDrinking_No  \\\n",
       "0        5.0          0.0         1.0                 1.0   \n",
       "1        7.0          1.0         0.0                 1.0   \n",
       "2        8.0          0.0         1.0                 1.0   \n",
       "3        6.0          1.0         0.0                 1.0   \n",
       "4        8.0          1.0         0.0                 1.0   \n",
       "\n",
       "   AlcoholDrinking_Yes  Stroke_No  Stroke_Yes  DiffWalking_No  \\\n",
       "0                  0.0        1.0         0.0             1.0   \n",
       "1                  0.0        0.0         1.0             1.0   \n",
       "2                  0.0        1.0         0.0             1.0   \n",
       "3                  0.0        1.0         0.0             1.0   \n",
       "4                  0.0        1.0         0.0             0.0   \n",
       "\n",
       "   DiffWalking_Yes  Sex_Female  Sex_Male  Race_White  Race_Black  Race_Asian  \\\n",
       "0              0.0         1.0       0.0         0.0         0.0         0.0   \n",
       "1              0.0         1.0       0.0         0.0         0.0         0.0   \n",
       "2              0.0         0.0       1.0         0.0         0.0         0.0   \n",
       "3              0.0         1.0       0.0         0.0         0.0         0.0   \n",
       "4              1.0         1.0       0.0         0.0         0.0         0.0   \n",
       "\n",
       "   Race_American Indian/Alaskan Native  Race_Other  Race_Hispanic  \\\n",
       "0                                  0.0         0.0            1.0   \n",
       "1                                  0.0         0.0            1.0   \n",
       "2                                  0.0         0.0            1.0   \n",
       "3                                  0.0         0.0            1.0   \n",
       "4                                  0.0         0.0            1.0   \n",
       "\n",
       "   Diabetic_Yes  Diabetic_No  Diabetic_No, borderline diabetes  \\\n",
       "0           0.0          0.0                               1.0   \n",
       "1           1.0          0.0                               0.0   \n",
       "2           0.0          0.0                               1.0   \n",
       "3           1.0          0.0                               0.0   \n",
       "4           1.0          0.0                               0.0   \n",
       "\n",
       "   Diabetic_Yes (during pregnancy)  PhysicalActivity_Yes  PhysicalActivity_No  \\\n",
       "0                              0.0                   0.0                  1.0   \n",
       "1                              0.0                   0.0                  1.0   \n",
       "2                              0.0                   0.0                  1.0   \n",
       "3                              0.0                   1.0                  0.0   \n",
       "4                              0.0                   0.0                  1.0   \n",
       "\n",
       "   Asthma_Yes  Asthma_No  KidneyDisease_No  KidneyDisease_Yes  SkinCancer_Yes  \\\n",
       "0         0.0        1.0               1.0                0.0             0.0   \n",
       "1         1.0        0.0               1.0                0.0             1.0   \n",
       "2         0.0        1.0               1.0                0.0             1.0   \n",
       "3         1.0        0.0               1.0                0.0             0.0   \n",
       "4         1.0        0.0               1.0                0.0             1.0   \n",
       "\n",
       "   SkinCancer_No  \n",
       "0            1.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            1.0  \n",
       "4            0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data as a dataframe\n",
    "data = pd.read_csv(\"data/raw_data.csv\")\n",
    "\n",
    "# Create a lists of each type of feature\n",
    "nominal_features = ['Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex', 'Race', 'Diabetic',\n",
    "                  'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer']\n",
    "ordinal_features = ['AgeCategory', 'GenHealth']\n",
    "continuous_features = ['BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime']\n",
    "\n",
    "\n",
    "# region Convert the nominal features into one-hot encodings\n",
    "\n",
    "# Create a One-Hot Encoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# For each nominal feature...\n",
    "for feature in nominal_features:\n",
    "\n",
    "    # Get an encoded version\n",
    "    encoded_feature = pd.DataFrame(encoder.fit_transform(data[[feature]]).toarray(),\n",
    "                                   columns=[f'{feature}_{f_class}' for f_class in data[feature].unique()])\n",
    "\n",
    "    # Remove the old feature from the data\n",
    "    data = data.drop(feature, axis=1)\n",
    "\n",
    "    # Add the encoded feature to the data\n",
    "    data = data.join(encoded_feature)\n",
    "\n",
    "# endregion Convert the nominal features into one-hot encodings\n",
    "\n",
    "# region Convert the ordinal features into labels\n",
    "\n",
    "# For each nominal feature...\n",
    "for feature in ordinal_features:\n",
    "\n",
    "    # Replace the old feature with an encoded feature\n",
    "    data[feature] = data[feature].astype('category').cat.codes\n",
    "\n",
    "# endregion Convert the ordinal features into labels\n",
    "\n",
    "# Convert the output column to be numerical\n",
    "data['HeartDisease'] = data['HeartDisease'].astype('category').cat.codes\n",
    "\n",
    "# Display the number of columns in the dataframe\n",
    "print(f\"Number of Raw Features: {len(data.columns)}\")\n",
    "print()\n",
    "print(f\"Number of Datapoints: {data.shape[0]}\")\n",
    "print()\n",
    "\n",
    "# Display the head of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "To make the dataset trainable, we also used one-hot encoding on each of the nominal features. Additionally, we also\n",
    "changed each ordinal feature to be represented by ordered numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Heart Disease\n",
    "\n",
    "We are trying to predict Heart Disease with our dataset. However, the number of cases of heart disease are not equal to\n",
    "the number of cases without heart disease. Instead, only around 10% of people in the dataset have a heart disease.\n",
    "Therefore, we should expect a 10% accuracy if we guessed randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will use 3 different metrics to assess the performance of our models on our task of Heart Disease\n",
    "Prediction/Classification.\n",
    "\n",
    "The first metric we will use is **Recall/Sensitivity** (formula shown below). We will use this metric to assess\n",
    "performance of our task because recall measures the model's ability to classify the true positives in its predictions.\n",
    "This is relevant to the problem we are solving because we want our model to miss as few truly heart disease-prone\n",
    "individuals as possible. It is very expensive to miss a true positive (miss a heart disease prone patient) in this\n",
    "context.\n",
    "\n",
    "The second metric we will consider is **Precision** (formula shown below). We will use this metric to measure the\n",
    "performance of our model because it tells us how many predicted positives are actually positive. In the context of\n",
    "our problem, this metric answers the question: How many patients predicted to have heart disease actually had heart\n",
    "disease? Recall is a measure that can benefit from biasing predictions toward all true, so we use Precision to ensure\n",
    "that our model is still making balanced and accurate predictions.\n",
    "\n",
    "![Precision and Recall](assets/PrecisionRecall_formula.png)\n",
    "\n",
    "The last metric we will consider is **the F1 score** as it is the harmonic balance between Precision and Recall. We use F1 to make\n",
    "sure that our models predictions are measuring Prediction and Recall in a balanced manner, so that we assess if our\n",
    "model is biased towards predicting heart disease or non heart disease.\n",
    "\n",
    "![F1](assets/F1_formula.png)\n",
    "\n",
    "In the evaluation of our models, we will primarily be looking at Recall/Sensitivity, as it is the most relevant metric\n",
    "to the problem we are trying to address, which is the prediction/classification of heart disease. However, we will\n",
    "take the other two metrics into account as they provide information into the general accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Testing\n",
    "\n",
    "In order to experiment with our models and in the hopes of understanding how to use them before applying them to our\n",
    "real dataset, we created a model playground. With it, each of us trained our model on each of the different datasets,\n",
    "and learned a lot about what our model worked well at, and what it struggled to do.\n",
    "\n",
    "![GPC Example](assets/GPC_testing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hypotheses\n",
    "\n",
    "Hypothesis header\n",
    "\n",
    "### Gaussian Process Classifier\n",
    "*By Will Sumerfield*\n",
    "\n",
    "Given that GPC models perform very well on datasets with a good spread over the dataspace, and that our dataset is very\n",
    "large, I predict that the Gaussian Process Classifier model will perform very well, if I can find a good kernel\n",
    "function for the data. However, I expect that this will be a very difficult process, given that there are many\n",
    "columns of our data.\n",
    "\n",
    "Additionally, I may need to use smaller subsamples of the data to train the GPC, given that GPCs take up **$$O(n^2)$$**\n",
    "space, and take the same training time. I expect this model to be among, if not the best model we employ.\n",
    "\n",
    "### Support Vector Classifier\n",
    "*By Miguel Monares*\n",
    "\n",
    "Support Vector Machines (SVM) are effective in high dimensional spaces. Given that our data has decent number of\n",
    "features, we can expect these features to be a positive attribute for the SVM's performance on the dataset. The\n",
    "performance of the SVM works well when the data is mostly separable, but doesn't perform well when the dataset is\n",
    "overlapping. Hence, we can expect that heart disease classification performance of the SVM will depend heavily on\n",
    "whether the features can be effectively separated. It will be important that we use an appropriate kernel in order to\n",
    "get the best performance from this model.\n",
    "\n",
    "### Decision Tree Classifier\n",
    "*By Miguel Monares*\n",
    "\n",
    "Decision Tree's may be a useful model for our task of heart disease classification because Decision Tree's offer\n",
    "interpretability and visualization that may allow us to discover insights and connections between the features that are\n",
    "indicative of heart disease. However, in order to maximize our performance, we need to make sure that decision tree\n",
    "doesn't overfit to the data, which we can influence by controlling pruning and max-depth. I expect this model will be\n",
    "able to find underlying connection between the features in our data, but will not be among the best.\n",
    "\n",
    "### Random Forest Classification\n",
    "*By Abdalla Atalla*\n",
    "\n",
    "As the number of trees gets bigger, increases it will lead to no overfitting of the model which will be a benefit.\n",
    "Also, we will try to aim for low bias + correlation that will lead to a better accuracy. I expect that the RF will do\n",
    "great because of the accuracy, simplicity, and how computationally inexpensive it is to work with many features.\n",
    "Another positive to using RF is that it is good with big amounts of data and it also isn't too sensitive to the\n",
    "outliers in the data.\n",
    "\n",
    "### Neural Networks\n",
    "*By Ritik Raina*\n",
    "\n",
    "Techinally, we are working with a binary classification modulation - either one has or does not have a heart disease.\n",
    "Here, neural networks will be helpful when it comes to ensuring taht we apply weights and biases to each of the fitted\n",
    "features, whenever the training is in process. Using neural network architectures will open up the opportunity to work\n",
    "with activation functions which are extremely influential of the features and their values.\n",
    "\n",
    "### Logistic Regression\n",
    "*By Matilda Michel*\n",
    "\n",
    "Given the high dimensionality of the data, it’s hard to tell if the data will be linearly separable and if logistic\n",
    "regression will be able to accurately predict the data.\n",
    "\n",
    "With preprocessing the data into polynomial features of varying degrees, we can see how it performs on various higher\n",
    "orders to try and best fit the relationship of the data. Given the nature of our dataset, I expect a model with a\n",
    "higher degree polynomial will perform the best, though I'll need to be careful with overfitting. Another issue I might\n",
    "run into is having enough memory to run higher degrees of polynomial features in the model as the number of features\n",
    "exponentially increases with n original features.\n",
    "\n",
    "To conserve time, we can also use a polynomial kernel trick for logistic regression, and see how that fits the data on\n",
    "higher degrees. I’m expecting that this could perform just as well or better given that it's less computationally\n",
    "expensive so I might be able to tune the hyperparameters more freely.\n",
    "\n",
    "\n",
    "Hypothesis Footer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "We chose 7 different supervised machine learning models to explore our dataset with and to see how their performances\n",
    "compare. We individually explored these models and stated our own hypotheses on how well we think they will perform\n",
    "and why, given what we know about our data.\n",
    "\n",
    "Each metric here is based on predicting the positive case, since with a measure like heart disease, we care much more\n",
    "about finding unhealthy individuals, than finding health ones.\n",
    "\n",
    "### Gaussian Process Classifier\n",
    "*By Will Sumerfield*\n",
    "\n",
    "| Measure | Score |\n",
    "---|---\n",
    "Train Precision | 0.14285714285213123\n",
    "Train Recall |   0.10223642172523961\n",
    "Train F1 Score | 0.14541233333221455\n",
    "Test Precision | 0.08712890539028329\n",
    "Test Recall |    0.11923922938209103\n",
    "Test F1 Score |  0.09239812829483894\n",
    "\n",
    "### Support Vector Classifier\n",
    "*By Miguel Monares*\n",
    "\n",
    "| Measure | Score |\n",
    "---|---\n",
    "Train Precision | 0.16666666666666666\n",
    "Train Recall | 0.10223642172523961\n",
    "Train F1 Score | 0.12673267326732673\n",
    "Test Precision | 0.2777777777777778\n",
    "Test Recall | 0.16025641025641027\n",
    "Test F1 Score | 0.20325203252032523\n",
    "\n",
    "### Decision Tree Classifier\n",
    "*By Miguel Monares*\n",
    "\n",
    "| Measure | Score |\n",
    "---|---\n",
    "Train Precision | 1.0\n",
    "Train Recall | 0.9936102236421726\n",
    "Train F1 Score | 0.9967948717948718\n",
    "Test Precision |  0.18137254901960784\n",
    "Test Recall |  0.23717948717948717\n",
    "Test F1 Score | 0.20555555555555557\n",
    "\n",
    "### Random Forest Classification\n",
    "*By Abdalla Atalla*\n",
    "\n",
    "| Measure | Score |\n",
    "---|---\n",
    "Train Precision | 0.9330985915492958\n",
    "Train Recall | 0.8466453674121406\n",
    "Train F1 Score | 0.8877721943048575\n",
    "Test Precision | 0.35\n",
    "Test Recall | 0.1794871794871795\n",
    "Test F1 Score | 0.23728813559322035\n",
    "\n",
    "### Neural Networks\n",
    "*By Ritik Raina*\n",
    "\n",
    "| Measure | Score |\n",
    "---|---\n",
    "Train Precision | 0.5845070422535211\n",
    "Train Recall |   0.08186749958901858\n",
    "Train F1 Score | 0.1436193222782985\n",
    "Test Precision |   0.5670347003154574\n",
    "Test Recall |  0.0788031565103025\n",
    "Test F1 Score | 0.13837567359507313\n",
    "\n",
    "### Logistic Regression\n",
    "*By Matilda Michel*\n",
    "\n",
    "| Measure | Score |\n",
    "---|---\n",
    "Train Precision |  0.23917748917748918\n",
    "Train Recall | 0.7060702875399361\n",
    "Train F1 Score | 0.35731608730800324\n",
    "Test Precision |  0.2314410480349345\n",
    "Test Recall | 0.6794871794871795\n",
    "Test F1 Score | 0.3452768729641694\n",
    "\n",
    "\n",
    "### Summary\n",
    "The highest recall score is Logistic Regression w/ Polynomial Features at .679, the highest precision is  Random Forest at .35, and the highest F1 score award goes to Logistic Regression at .345.\n",
    "\n",
    "Overall, we can see that the logistic regression performed the best! This goes to show that simpler models can often\n",
    "perform better than complex ones, and that their faster training times and easier setup are often worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysis\n",
    "\n",
    "Below, first we individually analyze the results from our trained models, looking at how they performed on their own, looking especially at recall, and also how they performed relative to other models.\n",
    "\n",
    "\n",
    "### Gaussian Process Classifier\n",
    "*By Will Sumerfield*\n",
    "\n",
    "I learned a lot from training my model on this real dataset. First, I learned that Guassian Processes\n",
    "are very difficult to train. They take up huge amounts of space, and a large amount of time to train.\n",
    "\n",
    "I attempted to follow a guide on GPCs, specifically on how to create custom kernels on 1-hot-encoded encoded data.\n",
    "However, the SKLearn library made this difficult to do, and while I believe I had a working version, the training\n",
    "time when using a kernel for each feature becomes very large. I suspect that libraries made more specifically for GPCs\n",
    "would work better for this.\n",
    "\n",
    "I was correct in my hypothesis that GPCs require a lot of knowledge about the data, and that their hyperparameters\n",
    "(kernel functions) are really what make or break them - they're definitely not 'plug and chug' models.\n",
    "\n",
    "In comparison to other models, this model performed relatively poorly. However, I do still believe that more \n",
    "processing power and time to craft the kernels would allow this model to be one of the better ones. The question now\n",
    "remains - is it worth it?\n",
    "\n",
    "The kernel I found that worked best was the DotProduct()*RBF() kernel. This means that a combination of these two \n",
    "(multiplication represents 'and') of these kernels was the most optimal kernel type. \n",
    "Notably, the White Noise Kernel decreased performance, suggesting that our data was not very noisy, and that this \n",
    "actually underfit our model.\n",
    "\n",
    "### Support Vector Classifier\n",
    "*By Miguel Monares*\n",
    "\n",
    "The Support Vector Classification Model performed decently compared to the other models. The Precision and F1 Score outscored our baseline (.15), which indicates that this model is better suited at predicting the correct status of heart disease for each patient. It has a decent recall score compared to the other models, and is a model we may consider out of the models we compare to use in practice. Out of the kernel functions we chose to use, sigmoid performed the best. This is probably because sigmoid is best suited for binary tasks, and the data is not inherently linearly separable, so the other kernels don’t perform as well.\n",
    "\n",
    "### Decision Tree Classifier\n",
    "*By Miguel Monares*\n",
    "\n",
    "The decision tree classification model had an f1 score of .222, which outscores the baseline model f1 score (.15). Additionally, it has a higher precision score. It has a close f1 performance to that of the support vector classifier. However, the decision tree classifier has a higher recall (.256 vs .160). Since we place more importance on the value of the recall score vs precision/f1, we declare the the decision tree model is better suited for this task of prediction of heart disease. Out of the criterion functions we consider for measuring the quality of splits, the gini function outperforms entropy and log_loss, which can be explained since gini facors big partitions. Gini is also faster computationally, so we prefer to use this criterion in the field.      \n",
    "\n",
    "### Random Forest Classification\n",
    "*By Abdalla Atalla*\n",
    "\n",
    "For the Random Forest classification model we see that it did relatively well compared to our baseline model score, with an f1 score of .237. This tells us that the . The best parameter for n_estimators was closer to ‘5’ and that is what produced a good precision score of .350. The recall score of .179 which tells us that this is a very picky classifier because it is missing a lot of true positives and this is due to having a higher precision than low recall score. We will see how this model matches up with other models as this has the potential due to its relatively good precision & f1 score, but seeing how the recall score will match up will be interesting to observe.\n",
    "\n",
    "### Neural Networks\n",
    "*By Ritik Raina*\n",
    "\n",
    "### Logistic Regression\n",
    "*By Matilda Michel*\n",
    "\n",
    "## Limitations\n",
    "A major limitation a few of us had was that our models were very computationally expensive, in terms of memory and\n",
    "processing time. This included Logistic Regression w/ Polynomial Features, Guassian Process Regression, and SVCs.\n",
    "We had to create a smaller random subset of the data to be able to perform some of the tasks in a reasonable amount of\n",
    "time. Since our dataset was so large, some processing  was severely limited due to the amount of memory needed.\n",
    "\n",
    "When exploring different degrees of polynomial features, the highest degree the model could run on\n",
    "without crashing was 4, and even then it could only compare multiple hyperparameters in a gridsearch for up to degree 3.\n",
    "The GPC was also had serious issues with runtime, and Will was only able to test a very limited number of kernels - a\n",
    "serious limitation when kernel choice is a very experimental and dynamic process.\n",
    "\n",
    "The ability for our models to be more accurate may have also been restricted by the lack of positive instances of the\n",
    "predicated class compared to negatives, possibly making it harder for them to find the relationships within the data.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "After comparing the performance of several popular machine learning algorithms on our task of heart disease prediction, we find that, when training for predicting instances of the positive class, logistic regression with polynomial features performed the best on our Heart Disease dataset, when focusing on recall and F1 scores. With a degree of 2, that model resulted in a max F1 score of 0.345, while a degree of 4 resulted in a max recall score of 0.853. The other models performed in the ranges of about 0.10 - 0.23 for max recall scores while generally performing in the 0.20 range for max F1 scores.\n",
    "\n",
    "This work could help other researchers understand what kinds of models to consider for building practical heart disease classifying models in the field. Additionally, our findings could help give insight to the features that are most helpful for heart disease, or classification of other medical conditions/diseases, which may enable researchers to build more accurate models, maybe employing a combination of the models that were successful to employ a stronger ensemble model/highly optimized and tuned model.\n",
    "\n",
    "For future work, we may reexamine the models that performed better on our dataset and try to optimize the parameters and hyperparameters that we use in the model. Additionally, we might try to explore how an ensemble model, using our findings above, may improve our performance on the task further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ethics and Privacy\n",
    "\n",
    "When developing machine learning applications, it's important to be aware and proactive against ethics and privacy\n",
    "concerns. The dataset we use does not include Personal Identifiable Information (PII), which helps ensure that the\n",
    "privacy of patients is kept.\n",
    "\n",
    "Ethical concerns can arise in the results processing and analysis stage. It's important that the results are validated\n",
    "to make sure they are reasonable, so that possible correlations in the underlying data are pursued and perceived to be\n",
    "true. For example, if a correlation between race and diabetes is found, it's important to not jump to conclusions and\n",
    "declare causality/truth behind the correlation.\n",
    "\n",
    "Additionally, we need to make sure that the data we use to train our model is representative of the general population,\n",
    "so that we can best fit our model to classify heart disease in patients with different types of backgrounds .\n",
    "In the field of medicine, it's important that our product is tested and verified in the interest of liability. While it\n",
    "is important that we miss as few positively predicted heart disease patients in our model, we need to make sure that\n",
    "our model doesn't overly classify non heart disease prone patients as positive as this would lead to resources and\n",
    "facilities being directed toward those who don't need it.\n",
    "\n",
    "In the field, it is possible that this product may produce some unintended consequences relating to ethics or privacy.\n",
    "If this product is released into the field, we will make sure to address any unintended outcomes breaching ethical or\n",
    " privacy concerns by working to understand why such breaches occurred and being proactive about alleviating such\n",
    " concerns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}