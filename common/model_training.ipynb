{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training and Testing\n",
    "\n",
    "In this file, we train each of our models on the exact same training dataset, and test them\n",
    "on the same testing dataset. This allows us to judge their performance, strengths, and\n",
    "weaknesses using a comparable measure. We hope that this will, in conjunction with our\n",
    "hypotheses, help us understand the merits of each model, and be more informed in future\n",
    "instances of model selection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support as class_metrics\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the Dataset\n",
    "\n",
    "First, we separate out the testing and training dataset for each of our models to train on.\n",
    "In this case, we decided against k-cross validation due to the sheer size of our dataset.\n",
    "The accuracy of our models should be representative of their performance on future data as\n",
    "well, since there is such a large quantity of training data.\n",
    "\n",
    "Our dataset does have a few issues: several of the variables have classes which occur very infrequently.\n",
    "This may bias our results, since we don't know much about how that variable affects heart disease relative to the other\n",
    "classes. Therefore, we may need to stratify our data to get more accurate results.\n",
    "Additionally, the columns **GenHealth** and **Stroke** have the potential to bias our results.\n",
    "These factors are much more likely to occur due to heart disease, but may not be a good predictor\n",
    "of its future fruition. Therefore, to deal with each of these issues, we will make separate versions of our dataset for\n",
    "each of these cases: **Normal**, **Stratified**, and **No Health Indicators**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:     319795\n",
      "Columns:  35\n"
     ]
    },
    {
     "data": {
      "text/plain": "   HeartDisease    BMI  PhysicalHealth  MentalHealth  AgeCategory  GenHealth  \\\n0             0  16.60             3.0          30.0            7          4   \n1             0  20.34             0.0           0.0           12          4   \n2             0  26.58            20.0          30.0            9          1   \n3             0  24.21             0.0           0.0           11          2   \n4             0  23.71            28.0           0.0            4          4   \n\n   SleepTime  Smoking_Yes  Smoking_No  AlcoholDrinking_No  \\\n0        5.0          0.0         1.0                 1.0   \n1        7.0          1.0         0.0                 1.0   \n2        8.0          0.0         1.0                 1.0   \n3        6.0          1.0         0.0                 1.0   \n4        8.0          1.0         0.0                 1.0   \n\n   AlcoholDrinking_Yes  Stroke_No  Stroke_Yes  DiffWalking_No  \\\n0                  0.0        1.0         0.0             1.0   \n1                  0.0        0.0         1.0             1.0   \n2                  0.0        1.0         0.0             1.0   \n3                  0.0        1.0         0.0             1.0   \n4                  0.0        1.0         0.0             0.0   \n\n   DiffWalking_Yes  Sex_Female  Sex_Male  Race_White  Race_Black  Race_Asian  \\\n0              0.0         1.0       0.0         0.0         0.0         0.0   \n1              0.0         1.0       0.0         0.0         0.0         0.0   \n2              0.0         0.0       1.0         0.0         0.0         0.0   \n3              0.0         1.0       0.0         0.0         0.0         0.0   \n4              1.0         1.0       0.0         0.0         0.0         0.0   \n\n   Race_American Indian/Alaskan Native  Race_Other  Race_Hispanic  \\\n0                                  0.0         0.0            1.0   \n1                                  0.0         0.0            1.0   \n2                                  0.0         0.0            1.0   \n3                                  0.0         0.0            1.0   \n4                                  0.0         0.0            1.0   \n\n   Diabetic_Yes  Diabetic_No  Diabetic_No, borderline diabetes  \\\n0           0.0          0.0                               1.0   \n1           1.0          0.0                               0.0   \n2           0.0          0.0                               1.0   \n3           1.0          0.0                               0.0   \n4           1.0          0.0                               0.0   \n\n   Diabetic_Yes (during pregnancy)  PhysicalActivity_Yes  PhysicalActivity_No  \\\n0                              0.0                   0.0                  1.0   \n1                              0.0                   0.0                  1.0   \n2                              0.0                   0.0                  1.0   \n3                              0.0                   1.0                  0.0   \n4                              0.0                   0.0                  1.0   \n\n   Asthma_Yes  Asthma_No  KidneyDisease_No  KidneyDisease_Yes  SkinCancer_Yes  \\\n0         0.0        1.0               1.0                0.0             0.0   \n1         1.0        0.0               1.0                0.0             1.0   \n2         0.0        1.0               1.0                0.0             1.0   \n3         1.0        0.0               1.0                0.0             0.0   \n4         1.0        0.0               1.0                0.0             1.0   \n\n   SkinCancer_No  \n0            1.0  \n1            0.0  \n2            0.0  \n3            1.0  \n4            0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HeartDisease</th>\n      <th>BMI</th>\n      <th>PhysicalHealth</th>\n      <th>MentalHealth</th>\n      <th>AgeCategory</th>\n      <th>GenHealth</th>\n      <th>SleepTime</th>\n      <th>Smoking_Yes</th>\n      <th>Smoking_No</th>\n      <th>AlcoholDrinking_No</th>\n      <th>AlcoholDrinking_Yes</th>\n      <th>Stroke_No</th>\n      <th>Stroke_Yes</th>\n      <th>DiffWalking_No</th>\n      <th>DiffWalking_Yes</th>\n      <th>Sex_Female</th>\n      <th>Sex_Male</th>\n      <th>Race_White</th>\n      <th>Race_Black</th>\n      <th>Race_Asian</th>\n      <th>Race_American Indian/Alaskan Native</th>\n      <th>Race_Other</th>\n      <th>Race_Hispanic</th>\n      <th>Diabetic_Yes</th>\n      <th>Diabetic_No</th>\n      <th>Diabetic_No, borderline diabetes</th>\n      <th>Diabetic_Yes (during pregnancy)</th>\n      <th>PhysicalActivity_Yes</th>\n      <th>PhysicalActivity_No</th>\n      <th>Asthma_Yes</th>\n      <th>Asthma_No</th>\n      <th>KidneyDisease_No</th>\n      <th>KidneyDisease_Yes</th>\n      <th>SkinCancer_Yes</th>\n      <th>SkinCancer_No</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>16.60</td>\n      <td>3.0</td>\n      <td>30.0</td>\n      <td>7</td>\n      <td>4</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>20.34</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12</td>\n      <td>4</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>26.58</td>\n      <td>20.0</td>\n      <td>30.0</td>\n      <td>9</td>\n      <td>1</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>24.21</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>11</td>\n      <td>2</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>23.71</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset\n",
    "data = pd.read_csv('../data/raw_data.csv')\n",
    "\n",
    "# Create a lists of each type of feature\n",
    "nominal_features = ['Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex', 'Race', 'Diabetic',\n",
    "                  'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer']\n",
    "ordinal_features = ['AgeCategory', 'GenHealth']\n",
    "continuous_features = ['BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime']\n",
    "\n",
    "# region Convert the nominal features into one-hot encodings\n",
    "\n",
    "# Create a One-Hot Encoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# For each nominal feature...\n",
    "for feature in nominal_features:\n",
    "\n",
    "    # Get an encoded version\n",
    "    encoded_feature = pd.DataFrame(encoder.fit_transform(data[[feature]]).toarray(),\n",
    "                                   columns=[f'{feature}_{f_class}' for f_class in data[feature].unique()])\n",
    "\n",
    "    # Remove the old feature from the data\n",
    "    data = data.drop(feature, axis=1)\n",
    "\n",
    "    # Add the encoded feature to the data\n",
    "    data = data.join(encoded_feature)\n",
    "\n",
    "# endregion Convert the nominal features into one-hot encodings\n",
    "\n",
    "# region Convert the ordinal features into labels\n",
    "\n",
    "# For each nominal feature...\n",
    "for feature in ordinal_features:\n",
    "\n",
    "    # Replace the old feature with an encoded feature\n",
    "    data[feature] = data[feature].astype('category').cat.codes\n",
    "\n",
    "# endregion Convert the ordinal features into labels\n",
    "\n",
    "# Convert the output column to be numerical\n",
    "data['HeartDisease'] = data['HeartDisease'].astype('category').cat.codes\n",
    "\n",
    "# Get the X and y of the dataset\n",
    "data_X = data.iloc[:,1:]\n",
    "data_y = data.loc[:,'HeartDisease']\n",
    "\n",
    "# Display the head of the data\n",
    "print(\"Rows:    \", data.shape[0])\n",
    "print(\"Columns: \", data.shape[1])\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Get the Normal test-train split\n",
    "normal_train_X, normal_test_X, normal_train_y, normal_test_y = train_test_split(data_X, data_y, train_size=2/3,\n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Get the Stratified test-train split\n",
    "stratified_train_X, stratified_test_X, stratified_train_y, stratified_test_y = train_test_split(data_X, data_y,\n",
    "                                                                                                train_size=2/3,\n",
    "                                                                                                stratify=data_y,\n",
    "                                                                                                random_state=42)\n",
    "# Get the No Health test-train split\n",
    "dropped_features = [feature for feature in data.columns if ('GenHealth' in feature or 'Stroke' in feature) and feature in data_X.columns]\n",
    "nohealth_train_X, nohealth_test_X, nohealth_train_y, nohealth_test_y = train_test_split(data_X.drop(dropped_features,\n",
    "                                                                                                    axis=1),\n",
    "                                                                                        data_y, train_size=2/3,\n",
    "                                                                                        random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Tuning\n",
    "\n",
    "Now that we know have our datasets, we need to find which we want to use. To do that, we can introduce a simple model\n",
    "which we will use as a measure. By comparing the results of the model trained on each of the different datasets, we\n",
    "can get a sense of how the changes impact performance, and potentially help us intuit whether any bias is introduced by\n",
    "our data. In specific, we are looking to see if the dropped data columns or the stratification make the dataset perform\n",
    "better or worse.\n",
    "\n",
    "The model we will be using for this is **Logistic Regression**, since this model is very simple and fast to train."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\WILLSU~1\\AppData\\Local\\Temp/ipykernel_32168/3268466146.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mnormal_pipe\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnormal_train_X\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnormal_train_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mstratified_pipe\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstratified_train_X\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstratified_train_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[0mnohealth_pipe\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnohealth_train_X\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnohealth_train_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;31m# Make predictions with each logistic regression\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\pipeline.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    392\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_final_estimator\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;34m\"passthrough\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    393\u001B[0m                 \u001B[0mfit_params_last_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfit_params_steps\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 394\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_final_estimator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mXt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params_last_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    395\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    396\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1587\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1588\u001B[0m             \u001B[0mprefer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"processes\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1589\u001B[1;33m         fold_coefs_ = Parallel(\n\u001B[0m\u001B[0;32m   1590\u001B[0m             \u001B[0mn_jobs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1591\u001B[0m             \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1041\u001B[0m             \u001B[1;31m# remaining jobs.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1042\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterating\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1043\u001B[1;33m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdispatch_one_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1044\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterating\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_original_iterator\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1045\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36mdispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    860\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 861\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dispatch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtasks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    862\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    863\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36m_dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    778\u001B[0m             \u001B[0mjob_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jobs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 779\u001B[1;33m             \u001B[0mjob\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_async\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcallback\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    780\u001B[0m             \u001B[1;31m# A job can complete so quickly than its callback is\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    781\u001B[0m             \u001B[1;31m# called before we get here, causing self._jobs to\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py\u001B[0m in \u001B[0;36mapply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mapply_async\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcallback\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    207\u001B[0m         \u001B[1;34m\"\"\"Schedule a func to be run\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 208\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mImmediateResult\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    209\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcallback\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m             \u001B[0mcallback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    570\u001B[0m         \u001B[1;31m# Don't delay the application, to avoid keeping the input\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    571\u001B[0m         \u001B[1;31m# arguments in memory\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 572\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    573\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    574\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    260\u001B[0m         \u001B[1;31m# change the default number of processes to -1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    261\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mparallel_backend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_n_jobs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 262\u001B[1;33m             return [func(*args, **kwargs)\n\u001B[0m\u001B[0;32m    263\u001B[0m                     for func, args, kwargs in self.items]\n\u001B[0;32m    264\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    260\u001B[0m         \u001B[1;31m# change the default number of processes to -1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    261\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mparallel_backend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_n_jobs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 262\u001B[1;33m             return [func(*args, **kwargs)\n\u001B[0m\u001B[0;32m    263\u001B[0m                     for func, args, kwargs in self.items]\n\u001B[0;32m    264\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\fixes.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mconfig_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\u001B[0m in \u001B[0;36m_logistic_regression_path\u001B[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001B[0m\n\u001B[0;32m    804\u001B[0m                 \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msearchsorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    805\u001B[0m             ]\n\u001B[1;32m--> 806\u001B[1;33m             opt_res = optimize.minimize(\n\u001B[0m\u001B[0;32m    807\u001B[0m                 \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    808\u001B[0m                 \u001B[0mw0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_minimize.py\u001B[0m in \u001B[0;36mminimize\u001B[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001B[0m\n\u001B[0;32m    621\u001B[0m                                   **options)\n\u001B[0;32m    622\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mmeth\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'l-bfgs-b'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 623\u001B[1;33m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001B[0m\u001B[0;32m    624\u001B[0m                                 callback=callback, **options)\n\u001B[0;32m    625\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mmeth\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'tnc'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\lbfgsb.py\u001B[0m in \u001B[0;36m_minimize_lbfgsb\u001B[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001B[0m\n\u001B[0;32m    358\u001B[0m             \u001B[1;31m# until the completion of the current minimization iteration.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    359\u001B[0m             \u001B[1;31m# Overwrite f and g:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 360\u001B[1;33m             \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc_and_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    361\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mtask_str\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mb'NEW_X'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m             \u001B[1;31m# new iteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001B[0m in \u001B[0;36mfun_and_grad\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    265\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray_equal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    266\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_x_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 267\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_fun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    268\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    269\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001B[0m in \u001B[0;36m_update_fun\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    231\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_update_fun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    232\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_updated\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 233\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_fun_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    234\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_updated\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    235\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001B[0m in \u001B[0;36mupdate_fun\u001B[1;34m()\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 137\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfun_wrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    138\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_fun_impl\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001B[0m in \u001B[0;36mfun_wrapped\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    132\u001B[0m             \u001B[1;31m# Overwriting results in undefined behaviour because\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    133\u001B[0m             \u001B[1;31m# fun(self.x) will change self.x, with the two no longer linked.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 134\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    135\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mcopy\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\lib\\function_base.py\u001B[0m in \u001B[0;36mcopy\u001B[1;34m(a, order, subok)\u001B[0m\n\u001B[0;32m    802\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    803\u001B[0m     \"\"\"\n\u001B[1;32m--> 804\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0morder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msubok\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msubok\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    805\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    806\u001B[0m \u001B[1;31m# Basic operations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Create a standard scalar and one-hot-encoding pipeline for each dataset\n",
    "normal_pipe = Pipeline([('scalar', StandardScaler()), ('lr', LogisticRegression())])\n",
    "stratified_pipe = Pipeline([('scalar', StandardScaler()), ('lr', LogisticRegression())])\n",
    "nohealth_pipe = Pipeline([('scalar', StandardScaler()), ('lr', LogisticRegression())])\n",
    "\n",
    "# Train each logistic regression on its dataset\n",
    "normal_pipe.fit(normal_train_X, normal_train_y)\n",
    "stratified_pipe.fit(stratified_train_X, stratified_train_y)\n",
    "nohealth_pipe.fit(nohealth_train_X, nohealth_train_y)\n",
    "\n",
    "# Make predictions with each logistic regression\n",
    "normal_prediction = normal_pipe.predict(normal_test_X)\n",
    "stratified_prediction = stratified_pipe.predict(stratified_test_X)\n",
    "nohealth_prediction = nohealth_pipe.predict(nohealth_test_X)\n",
    "\n",
    "# Find the score of each regression\n",
    "normal_precision, normal_recall, normal_f, _ = class_metrics(normal_test_y, normal_prediction)\n",
    "stratified_precision, stratified_recall, stratified_f, _ = class_metrics(stratified_test_y, stratified_prediction)\n",
    "nohealth_precision, nohealth_recall, nohealth_f, _ = class_metrics(nohealth_test_y, nohealth_prediction)\n",
    "\n",
    "# Display the score of each regression\n",
    "print(\"Normal Dataset LR:\")\n",
    "print(f\"Precision: {normal_precision[1]}\")\n",
    "print(f\"Recall:    {normal_recall[1]}\")\n",
    "print(f\"F1 Score:  {normal_f[1]}\")\n",
    "print()\n",
    "print(\"Stratified Dataset LR:\")\n",
    "print(f\"Precision: {stratified_precision[1]}\")\n",
    "print(f\"Recall:    {stratified_recall[1]}\")\n",
    "print(f\"F1 Score:  {stratified_f[1]}\")\n",
    "print()\n",
    "print(\"No Health Dataset LR:\")\n",
    "print(f\"Precision: {nohealth_precision[1]}\")\n",
    "print(f\"Recall:    {nohealth_recall[1]}\")\n",
    "print(f\"F1 Score:  {nohealth_f[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the stratified dataset performed marginally better than the normal dataset, whereas the No Health\n",
    "indicators dataset performed moderately worse. If were really trying to predict heart disease, this would indicate to\n",
    "us that using these columns may be unhelpful for our predictions. However, since that is not the case and since we have\n",
    "the data, we are going to use it anyways, and keep with the stratified dataset.\n",
    "\n",
    "However, some of our models may need a smaller subset of the data, such as the Gaussian Process Classifier. This is\n",
    "because they have high memory demands which scale with the data.\n",
    "\n",
    "Below, let's do two things: Relabel the stratified dataset we'll be using to be more simple, and create a subset of that\n",
    "dataset for models which need less data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rename the stratified dataset\n",
    "train_X, test_X, train_y, test_y = stratified_train_X, stratified_test_X, stratified_train_y, stratified_test_y\n",
    "\n",
    "# Get a smaller subset of the dataset\n",
    "data_subset = data.sample(5000, random_state=42)\n",
    "data_sub_X = data_subset.iloc[:,1:]\n",
    "data_sub_y = data_subset.loc[:,'HeartDisease']\n",
    "\n",
    "# Get the Stratified test-train split\n",
    "train_sub_X, test_sub_X, train_sub_y, test_sub_y = train_test_split(data_sub_X, data_sub_y, train_size=2/3,\n",
    "                                                                    stratify=data_sub_y, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n",
    "\n",
    "Now, we need to train each of our models on the stratified data. Since our training dataset is so large (around\n",
    "200,000 samples), we do not need to cross validate our results for each model - they should be representative of true\n",
    "performance relative to one another.\n",
    "\n",
    "Let's start by defining some methods that makes sure that each of our models is trained the same general way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, parameter_dict, dataset_train_x, dataset_train_y):\n",
    "    \"\"\"This function is used to the find the best model using Cross Validating grid search.\n",
    "    It also applies a standard scaling pipeline to the data.\n",
    "\n",
    "    :param model: An SK Learn model to train\n",
    "    :param parameter_dict: A dictionary of parameters to optimize using grid search. Keys are non-pipe parameters.\n",
    "    :param dataset_train_x: The X values of the training dataset\n",
    "    :param dataset_train_y: The y values of the training dataset\n",
    "    :return best_model: The best model found by the grid search, retrained on all of the data\n",
    "    :return search: The grid search object\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([('scalar', StandardScaler()), ('model', model)])\n",
    "\n",
    "    # Find the parameter grid\n",
    "    parameter_grid = {'model__' + param: parameter_dict[param] for param in parameter_dict}\n",
    "\n",
    "    # Get the GridSearch object\n",
    "    search = GridSearchCV(pipe, parameter_grid, cv=3, n_jobs=-1, verbose=100, scoring='f1')\n",
    "\n",
    "    # Search for the best model\n",
    "    search.fit(dataset_train_x, dataset_train_y)\n",
    "\n",
    "    # Train the best model found\n",
    "    best_model = Pipeline([('scalar', StandardScaler()), ('model', model)]).set_params(**search.best_params_)\n",
    "    best_model.fit(dataset_train_x, dataset_train_y)\n",
    "\n",
    "    # Return the grid search object\n",
    "    return best_model, search\n",
    "\n",
    "\n",
    "def display_results(model_name, best_model, grid_search, dataset_train_x, dataset_train_y,\n",
    "                    dataset_test_x, dataset_test_y):\n",
    "    \"\"\"This function displays the results of the grid search both via tables and graphically.\n",
    "\n",
    "    :param model_name: The name of the SK Learn model trained\n",
    "    :param best_model: The best model found by the grid search\n",
    "    :param grid_search: The grid search object which optimized the model\n",
    "    :param dataset_train_x: The X values of the training dataset\n",
    "    :param dataset_train_y: The y values of the training dataset\n",
    "    :param dataset_test_x: The X values of the training dataset\n",
    "    :param dataset_test_y: The y values of the training dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add spacing\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Start by displaying the trend of the data over each of the parameters\n",
    "    for param in grid_search.best_params_:\n",
    "\n",
    "        # This dict stores the mean performance for each parameter\n",
    "        param_dict = {}\n",
    "\n",
    "        # For the parameter value for each grid search iteration\n",
    "        for p_ind, p in enumerate(grid_search.cv_results_[f'param_{param}']):\n",
    "\n",
    "            # Get P's name\n",
    "            pname = str(p)\n",
    "\n",
    "            # If the parameter is not in the param dictionary, add it\n",
    "            if pname not in param_dict: param_dict[pname] = (1, grid_search.cv_results_['mean_test_score'][p_ind])\n",
    "\n",
    "            # Otherwise, summate them\n",
    "            else: param_dict[pname] = (param_dict[pname][0] + 1,\n",
    "                                   param_dict[pname][1] + grid_search.cv_results_['mean_test_score'][p_ind])\n",
    "\n",
    "        # Finally, find the average value for each parameter value across all grid search iteration\n",
    "        for p in param_dict:\n",
    "            pname = str(p)\n",
    "            param_dict[pname] = param_dict[pname][1] / param_dict[pname][0]\n",
    "\n",
    "        # If the data is numeric, draw a lineplot\n",
    "        if type(list(param_dict.keys())[0]) is not str:\n",
    "            xtick_enum = range(len(list(param_dict.keys())))\n",
    "            ax = sns.lineplot(x=xtick_enum, y=param_dict.values())\n",
    "            ax.set_xticks(xtick_enum, list(param_dict.keys()))\n",
    "\n",
    "        # otherwise, draw a barplot\n",
    "        else: ax = sns.barplot(x=list(param_dict.keys()), y=list(param_dict.values()))\n",
    "\n",
    "        # Since we only care about the differences between models, zoom in the plot to see the differences\n",
    "        max_val = max(param_dict.values())\n",
    "        min_val = min(param_dict.values())\n",
    "        range_val = max_val - min_val\n",
    "        plt.ylim(min_val - range_val*0.1, max_val + range_val*0.1)\n",
    "\n",
    "        # Add plot aesthetics\n",
    "        ax.set_xlabel(param[7:].capitalize() + ' Value')\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        plt.title(f'F1 Score over {param[7:].capitalize()} Value')\n",
    "        plt.show()\n",
    "\n",
    "    # Calculate the performance of the best model\n",
    "    model_train_pred = best_model.predict(dataset_train_x)\n",
    "    model_test_pred = best_model.predict(dataset_test_x)\n",
    "\n",
    "    # Calculate the performance metrics of the best model\n",
    "    model_train_precision, model_train_recall, model_train_f1, _ = class_metrics(dataset_train_y, model_train_pred,\n",
    "                                                                                 zero_division=0)\n",
    "    model_test_precision, model_test_recall, model_test_f1, _ = class_metrics(dataset_test_y, model_test_pred,\n",
    "                                                                              zero_division=0)\n",
    "\n",
    "    # Display which parameters were best\n",
    "    print(\"The best parameters were:\")\n",
    "    for param in grid_search.best_params_:\n",
    "        print(re.search(r'model__(.*)', param).group(1) + f\": {grid_search.best_params_[param]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Display the scores of the best model\n",
    "    print(f\"{model_name.title()} Prediction Metrics:\")\n",
    "    print(f\"Train Precision: {model_train_precision[1]}\")\n",
    "    print(f\"Train Recall:    {model_train_recall[1]}\")\n",
    "    print(f\"Train F1 Score:  {model_train_f1[1]}\")\n",
    "    print(f\"Test Precision:  {model_test_precision[1]}\")\n",
    "    print(f\"Test Recall:     {model_test_recall[1]}\")\n",
    "    print(f\"Test F1 Score:   {model_test_f1[1]}\")\n",
    "\n",
    "\n",
    "def save_model(model_name, grid_search):\n",
    "    \"\"\"Save the best model's parameters.\n",
    "\n",
    "    :param model_name: The name of the SK Learn model trained\n",
    "    :param grid_search: The grid search object which optimized the model\n",
    "    \"\"\"\n",
    "\n",
    "    pickle.dump(grid_search.best_params_, open(f'../models/{model_name}.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model\n",
    "\n",
    "A good place to start is to make a baseline. The most simple baseline model we can use is one that always guesses that there is heart disease. Let's see how this performs, in order to compare it to our other models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataframe full of 'Yes' Heart Disease Predictions\n",
    "baseline_train_pred = pd.Series([1] * train_y.shape[0])\n",
    "baseline_test_pred = pd.Series([1] * test_y.shape[0])\n",
    "\n",
    "# Find the baseline classification metrics\n",
    "baseline_train_precision, baseline_train_recall, baseline_train_f, _ = class_metrics(train_y,\n",
    "                                                                                     baseline_train_pred,\n",
    "                                                                                     zero_division=0)\n",
    "baseline_test_precision, baseline_test_recall, baseline_test_f, _ = class_metrics(test_y,\n",
    "                                                                                  baseline_test_pred,\n",
    "                                                                                  zero_division=0)\n",
    "\n",
    "# Display the score of each regression\n",
    "print(\"Baseline Model Prediction Metrics:\")\n",
    "print(f\"Train Precision: {baseline_train_precision[1]}\")\n",
    "print(f\"Train Recall:    {baseline_train_recall[1]}\")\n",
    "print(f\"Train F1 Score:  {baseline_train_f[1]}\")\n",
    "print(f\"Test Precision:  {baseline_test_precision[1]}\")\n",
    "print(f\"Test Recall:     {baseline_test_recall[1]}\")\n",
    "print(f\"Test F1 Score:   {baseline_test_f[1]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, our model does pretty poorly overall. Although it does correctly label all occurrences of heart disease,\n",
    "it fails to label any of the negative cases. Our F Score to beat comes out to be **0.159**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian Process Classification\n",
    "*By Will Sumerfield*\n",
    "\n",
    "### Gaussian Process Classification Hypothesis\n",
    "\n",
    "Given that GPC models perform very well on datasets with a good spread over the dataspace, and that our dataset is very\n",
    "large, I predict that the Gaussian Process Classifier model will perform very well, if I can find a good kernel\n",
    "function for the data. However, I expect that this will be a very difficult process, given that there are many\n",
    "columns of our data.\n",
    "\n",
    "Additionally, I may need to use smaller subsamples of the data to train the GPC, given that GPCs take up **$O(n^2)$**\n",
    "space, and take the same training time. I expect this model to be among, if not the best model we employ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of potential parameters\n",
    "gpc_params = {'kernel': [RBF(), WhiteKernel() + RBF()]}\n",
    "\n",
    "# Train the model, and find the best model from the provided inputs\n",
    "best_model, grid_search = train_model(GaussianProcessClassifier(random_state=42, copy_X_train=False), gpc_params,\n",
    "                                      train_sub_X, train_sub_y)\n",
    "\n",
    "# Display the results\n",
    "display_results(\"Gaussian Process Classifier\", best_model, grid_search, train_sub_X, train_sub_y, test_sub_X, test_sub_y)\n",
    "\n",
    "# Save the model\n",
    "save_model(\"Gaussian Process Classifier\", grid_search)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gaussian Process Classification Analysis\n",
    "\n",
    "Put a summary of your model's performance in this spot!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Copy-Paste all the GPC stuff here! Don't run the entire notebook, or you will train each model. That will take ages...\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Group041-Sp22)",
   "language": "python",
   "name": "pycharm-b7952dfa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}