{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training and Testing\n",
    "\n",
    "In this file, we train each of our models on the exact same training dataset, and test them\n",
    "on the same testing dataset. This allows us to judge their performance, strengths, and\n",
    "weaknesses using a comparable measure. We hope that this will, in conjunction with our\n",
    "hypotheses, help us understand the merits of each model, and be more informed in future\n",
    "instances of model selection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support as class_metrics\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the Dataset\n",
    "\n",
    "First, we separate out the testing and training dataset for each of our models to train on.\n",
    "In this case, we decided against k-cross validation due to the sheer size of our dataset.\n",
    "The accuracy of our models should be representative of their performance on future data as\n",
    "well, since there is such a large quantity of training data.\n",
    "\n",
    "Our dataset does have a few issues: several of the variables have classes which occur very infrequently.\n",
    "This may bias our results, since we don't know much about how that variable affects heart disease relative to the other\n",
    "classes. Therefore, we may need to stratify our data to get more accurate results.\n",
    "Additionally, the columns **GenHealth** and **Stroke** have the potential to bias our results.\n",
    "These factors are much more likely to occur due to heart disease, but may not be a good predictor\n",
    "of its future fruition. Therefore, to deal with each of these issues, we will make separate versions of our dataset for\n",
    "each of these cases: **Normal**, **Stratified**, and **No Health Indicators**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "data = pd.read_csv('../data/raw_data.csv')\n",
    "\n",
    "# Create a lists of each type of feature\n",
    "nominal_features = ['Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex', 'Race', 'Diabetic',\n",
    "                  'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer']\n",
    "ordinal_features = ['AgeCategory', 'GenHealth']\n",
    "continuous_features = ['BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime']\n",
    "\n",
    "# region Convert the nominal features into one-hot encodings\n",
    "\n",
    "# Create a One-Hot Encoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# For each nominal feature...\n",
    "for feature in nominal_features:\n",
    "\n",
    "    # Get an encoded version\n",
    "    encoded_feature = pd.DataFrame(encoder.fit_transform(data[[feature]]).toarray(),\n",
    "                                   columns=[f'{feature}_{f_class}' for f_class in data[feature].unique()])\n",
    "\n",
    "    # Remove the old feature from the data\n",
    "    data = data.drop(feature, axis=1)\n",
    "\n",
    "    # Add the encoded feature to the data\n",
    "    data = data.join(encoded_feature)\n",
    "\n",
    "# endregion Convert the nominal features into one-hot encodings\n",
    "\n",
    "# region Convert the ordinal features into labels\n",
    "\n",
    "# For each nominal feature...\n",
    "for feature in ordinal_features:\n",
    "\n",
    "    # Replace the old feature with an encoded feature\n",
    "    data[feature] = data[feature].astype('category').cat.codes\n",
    "\n",
    "# endregion Convert the ordinal features into labels\n",
    "\n",
    "# Convert the output column to be numerical\n",
    "data['HeartDisease'] = data['HeartDisease'].astype('category').cat.codes\n",
    "\n",
    "# Get the X and y of the dataset\n",
    "data_X = data.iloc[:,1:]\n",
    "data_y = data.loc[:,'HeartDisease']\n",
    "\n",
    "# Display the head of the data\n",
    "data.head()\n",
    "print(\"Rows:    \", data.shape[0])\n",
    "print(\"Columns: \", data.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the Normal test-train split\n",
    "normal_train_X, normal_test_X, normal_train_y, normal_test_y = train_test_split(data_X, data_y, train_size=2/3,\n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Get the Stratified test-train split\n",
    "stratified_train_X, stratified_test_X, stratified_train_y, stratified_test_y = train_test_split(data_X, data_y,\n",
    "                                                                                                train_size=2/3,\n",
    "                                                                                                stratify=data_y,\n",
    "                                                                                                random_state=42)\n",
    "# Get the No Health test-train split\n",
    "dropped_features = [feature for feature in data.columns if ('GenHealth' in feature or 'Stroke' in feature) and feature in data_X.columns]\n",
    "nohealth_train_X, nohealth_test_X, nohealth_train_y, nohealth_test_y = train_test_split(data_X.drop(dropped_features,\n",
    "                                                                                                    axis=1),\n",
    "                                                                                        data_y, train_size=2/3,\n",
    "                                                                                        random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Tuning\n",
    "\n",
    "Now that we know have our datasets, we need to find which we want to use. To do that, we can introduce a simple model\n",
    "which we will use as a measure. By comparing the results of the model trained on each of the different datasets, we\n",
    "can get a sense of how the changes impact performance, and potentially help us intuit whether any bias is introduced by\n",
    "our data. In specific, we are looking to see if the dropped data columns or the stratification make the dataset perform\n",
    "better or worse.\n",
    "\n",
    "The model we will be using for this is **Logistic Regression**, since this model is very simple and fast to train."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a standard scalar and one-hot-encoding pipeline for each dataset\n",
    "normal_pipe = Pipeline([('scalar', StandardScaler()), ('lr', LogisticRegression())])\n",
    "stratified_pipe = Pipeline([('scalar', StandardScaler()), ('lr', LogisticRegression())])\n",
    "nohealth_pipe = Pipeline([('scalar', StandardScaler()), ('lr', LogisticRegression())])\n",
    "\n",
    "# Train each logistic regression on its dataset\n",
    "normal_pipe.fit(normal_train_X, normal_train_y)\n",
    "stratified_pipe.fit(stratified_train_X, stratified_train_y)\n",
    "nohealth_pipe.fit(nohealth_train_X, nohealth_train_y)\n",
    "\n",
    "# Make predictions with each logistic regression\n",
    "normal_prediction = normal_pipe.predict(normal_test_X)\n",
    "stratified_prediction = stratified_pipe.predict(stratified_test_X)\n",
    "nohealth_prediction = nohealth_pipe.predict(nohealth_test_X)\n",
    "\n",
    "# Find the score of each regression\n",
    "normal_precision, normal_recall, normal_f, _ = class_metrics(normal_test_y, normal_prediction)\n",
    "stratified_precision, stratified_recall, stratified_f, _ = class_metrics(stratified_test_y, stratified_prediction)\n",
    "nohealth_precision, nohealth_recall, nohealth_f, _ = class_metrics(nohealth_test_y, nohealth_prediction)\n",
    "\n",
    "# Display the score of each regression\n",
    "print(\"Normal Dataset LR:\")\n",
    "print(f\"Precision: {normal_precision[1]}\")\n",
    "print(f\"Recall:    {normal_recall[1]}\")\n",
    "print(f\"F1 Score:  {normal_f[1]}\")\n",
    "print()\n",
    "print(\"Stratified Dataset LR:\")\n",
    "print(f\"Precision: {stratified_precision[1]}\")\n",
    "print(f\"Recall:    {stratified_recall[1]}\")\n",
    "print(f\"F1 Score:  {stratified_f[1]}\")\n",
    "print()\n",
    "print(\"No Health Dataset LR:\")\n",
    "print(f\"Precision: {nohealth_precision[1]}\")\n",
    "print(f\"Recall:    {nohealth_recall[1]}\")\n",
    "print(f\"F1 Score:  {nohealth_f[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the stratified dataset performed marginally better than the normal dataset, whereas the No Health\n",
    "indicators dataset performed moderately worse. If were really trying to predict heart disease, this would indicate to\n",
    "us that using these columns may be unhelpful for our predictions. However, since that is not the case and since we have\n",
    "the data, we are going to use it anyways, and keep with the stratified dataset.\n",
    "\n",
    "However, some of our models may need a smaller subset of the data, such as the Gaussian Process Classifier. This is\n",
    "because they have high memory demands which scale with the data.\n",
    "\n",
    "Below, let's do two things: Relabel the stratified dataset we'll be using to be more simple, and create a subset of that\n",
    "dataset for models which need less data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rename the stratified dataset\n",
    "train_X, test_X, train_y, test_y = stratified_train_X, stratified_test_X, stratified_train_y, stratified_test_y\n",
    "\n",
    "# Get a smaller subset of the dataset\n",
    "data_subset = data.sample(5000, random_state=42)\n",
    "data_sub_X = data_subset.iloc[:,1:]\n",
    "data_sub_y = data_subset.loc[:,'HeartDisease']\n",
    "\n",
    "# Get the Stratified test-train split\n",
    "train_sub_X, test_sub_X, train_sub_y, test_sub_y = train_test_split(data_sub_X, data_sub_y, train_size=2/3,\n",
    "                                                                    stratify=data_sub_y, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n",
    "\n",
    "Now, we need to train each of our models on the stratified data. Since our training dataset is so large (around\n",
    "200,000 samples), we do not need to cross validate our results for each model - they should be representative of true\n",
    "performance relative to one another.\n",
    "\n",
    "Let's start by defining some methods that makes sure that each of our models is trained the same general way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, parameter_dict, dataset_train_x, dataset_train_y):\n",
    "    \"\"\"This function is used to the find the best model using Cross Validating grid search.\n",
    "    It also applies a standard scaling pipeline to the data.\n",
    "\n",
    "    :param model: An SK Learn model to train\n",
    "    :param parameter_dict: A dictionary of parameters to optimize using grid search. Keys are non-pipe parameters.\n",
    "    :param dataset_train_x: The X values of the training dataset\n",
    "    :param dataset_train_y: The y values of the training dataset\n",
    "    :return best_model: The best model found by the grid search, retrained on all of the data\n",
    "    :return search: The grid search object\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([('scalar', StandardScaler()), ('model', model)])\n",
    "\n",
    "    # Find the parameter grid\n",
    "    parameter_grid = {'model__' + param: parameter_dict[param] for param in parameter_dict}\n",
    "\n",
    "    # Get the GridSearch object\n",
    "    search = GridSearchCV(pipe, parameter_grid, cv=3, n_jobs=-1, verbose=100, scoring='f1')\n",
    "\n",
    "    # Search for the best model\n",
    "    search.fit(dataset_train_x, dataset_train_y)\n",
    "\n",
    "    # Train the best model found\n",
    "    best_model = Pipeline([('scalar', StandardScaler()), ('model', model)]).set_params(**search.best_params_)\n",
    "    best_model.fit(dataset_train_x, dataset_train_y)\n",
    "\n",
    "    # Return the grid search object\n",
    "    return best_model, search\n",
    "\n",
    "\n",
    "def display_results(model_name, best_model, grid_search, dataset_train_x, dataset_train_y,\n",
    "                    dataset_test_x, dataset_test_y):\n",
    "    \"\"\"This function displays the results of the grid search both via tables and graphically.\n",
    "\n",
    "    :param model_name: The name of the SK Learn model trained\n",
    "    :param best_model: The best model found by the grid search\n",
    "    :param grid_search: The grid search object which optimized the model\n",
    "    :param dataset_train_x: The X values of the training dataset\n",
    "    :param dataset_train_y: The y values of the training dataset\n",
    "    :param dataset_test_x: The X values of the training dataset\n",
    "    :param dataset_test_y: The y values of the training dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add spacing\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Start by displaying the trend of the data over each of the parameters\n",
    "    for param in grid_search.best_params_:\n",
    "\n",
    "        # This dict stores the mean performance for each parameter\n",
    "        param_dict = {}\n",
    "\n",
    "        # For the parameter value for each grid search iteration\n",
    "        for p_ind, p in enumerate(grid_search.cv_results_[f'param_{param}']):\n",
    "\n",
    "            # Get P's name\n",
    "            pname = str(p)\n",
    "\n",
    "            # If the parameter is not in the param dictionary, add it\n",
    "            if pname not in param_dict: param_dict[pname] = (1, grid_search.cv_results_['mean_test_score'][p_ind])\n",
    "\n",
    "            # Otherwise, summate them\n",
    "            else: param_dict[pname] = (param_dict[pname][0] + 1,\n",
    "                                   param_dict[pname][1] + grid_search.cv_results_['mean_test_score'][p_ind])\n",
    "\n",
    "        # Finally, find the average value for each parameter value across all grid search iteration\n",
    "        for p in param_dict:\n",
    "            pname = str(p)\n",
    "            param_dict[pname] = param_dict[pname][1] / param_dict[pname][0]\n",
    "\n",
    "        # If the data is numeric, draw a lineplot\n",
    "        if type(list(param_dict.keys())[0]) is not str:\n",
    "            xtick_enum = range(len(list(param_dict.keys())))\n",
    "            ax = sns.lineplot(x=xtick_enum, y=param_dict.values())\n",
    "            ax.set_xticks(xtick_enum, list(param_dict.keys()))\n",
    "\n",
    "        # otherwise, draw a barplot\n",
    "        else: ax = sns.barplot(x=list(param_dict.keys()), y=list(param_dict.values()))\n",
    "\n",
    "        # Since we only care about the differences between models, zoom in the plot to see the differences\n",
    "        max_val = max(param_dict.values())\n",
    "        min_val = min(param_dict.values())\n",
    "        range_val = max_val - min_val\n",
    "        plt.ylim(min_val - range_val*0.1, max_val + range_val*0.1)\n",
    "\n",
    "        # Add plot aesthetics\n",
    "        ax.set_xlabel(param[7:].capitalize() + ' Value')\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        plt.title(f'F1 Score over {param[7:].capitalize()} Value')\n",
    "        plt.show()\n",
    "\n",
    "    # Calculate the performance of the best model\n",
    "    model_train_pred = best_model.predict(dataset_train_x)\n",
    "    model_test_pred = best_model.predict(dataset_test_x)\n",
    "\n",
    "    # Calculate the performance metrics of the best model\n",
    "    model_train_precision, model_train_recall, model_train_f1, _ = class_metrics(dataset_train_y, model_train_pred,\n",
    "                                                                                 zero_division=0)\n",
    "    model_test_precision, model_test_recall, model_test_f1, _ = class_metrics(dataset_test_y, model_test_pred,\n",
    "                                                                              zero_division=0)\n",
    "\n",
    "    # Display which parameters were best\n",
    "    print(\"The best parameters were:\")\n",
    "    for param in grid_search.best_params_:\n",
    "        print(re.search(r'model__(.*)', param).group(1) + f\": {grid_search.best_params_[param]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Display the scores of the best model\n",
    "    print(f\"{model_name.title()} Prediction Metrics:\")\n",
    "    print(f\"Train Precision: {model_train_precision[1]}\")\n",
    "    print(f\"Train Recall:    {model_train_recall[1]}\")\n",
    "    print(f\"Train F1 Score:  {model_train_f1[1]}\")\n",
    "    print(f\"Test Precision:  {model_test_precision[1]}\")\n",
    "    print(f\"Test Recall:     {model_test_recall[1]}\")\n",
    "    print(f\"Test F1 Score:   {model_test_f1[1]}\")\n",
    "\n",
    "\n",
    "def save_model(model_name, grid_search):\n",
    "    \"\"\"Save the best model's parameters.\n",
    "\n",
    "    :param model_name: The name of the SK Learn model trained\n",
    "    :param grid_search: The grid search object which optimized the model\n",
    "    \"\"\"\n",
    "\n",
    "    pickle.dump(grid_search.best_params_, open(f'../models/{model_name}.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model\n",
    "\n",
    "A good place to start is to make a baseline. The most simple baseline model we can use is one that always guesses that there is heart disease. Let's see how this performs, in order to compare it to our other models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataframe full of 'Yes' Heart Disease Predictions\n",
    "baseline_train_pred = pd.Series([1] * train_y.shape[0])\n",
    "baseline_test_pred = pd.Series([1] * test_y.shape[0])\n",
    "\n",
    "# Find the baseline classification metrics\n",
    "baseline_train_precision, baseline_train_recall, baseline_train_f, _ = class_metrics(train_y,\n",
    "                                                                                     baseline_train_pred,\n",
    "                                                                                     zero_division=0)\n",
    "baseline_test_precision, baseline_test_recall, baseline_test_f, _ = class_metrics(test_y,\n",
    "                                                                                  baseline_test_pred,\n",
    "                                                                                  zero_division=0)\n",
    "\n",
    "# Display the score of each regression\n",
    "print(\"Baseline Model Prediction Metrics:\")\n",
    "print(f\"Train Precision: {baseline_train_precision[1]}\")\n",
    "print(f\"Train Recall:    {baseline_train_recall[1]}\")\n",
    "print(f\"Train F1 Score:  {baseline_train_f[1]}\")\n",
    "print(f\"Test Precision:  {baseline_test_precision[1]}\")\n",
    "print(f\"Test Recall:     {baseline_test_recall[1]}\")\n",
    "print(f\"Test F1 Score:   {baseline_test_f[1]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, our model does pretty poorly overall. Although it does correctly label all occurrences of heart disease,\n",
    "it fails to label any of the negative cases. Our F Score to beat comes out to be **0.159**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian Process Classification\n",
    "*By Will Sumerfield*\n",
    "\n",
    "### Gaussian Process Classification Hypothesis\n",
    "\n",
    "Given that GPC models perform very well on datasets with a good spread over the dataspace, and that our dataset is very\n",
    "large, I predict that the Gaussian Process Classifier model will perform very well, if I can find a good kernel\n",
    "function for the data. However, I expect that this will be a very difficult process, given that there are many\n",
    "columns of our data.\n",
    "\n",
    "Additionally, I may need to use smaller subsamples of the data to train the GPC, given that GPCs take up **$O(n^2)$**\n",
    "space, and take the same training time. I expect this model to be among, if not the best model we employ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of potential parameters\n",
    "gpc_params = {'kernel': [RBF(), WhiteKernel() + RBF()]}\n",
    "\n",
    "# Train the model, and find the best model from the provided inputs\n",
    "best_model, grid_search = train_model(GaussianProcessClassifier(random_state=42, copy_X_train=False), gpc_params,\n",
    "                                      train_sub_X, train_sub_y)\n",
    "\n",
    "# Display the results\n",
    "display_results(\"Gaussian Process Classifier\", best_model, grid_search, train_sub_X, train_sub_y, test_sub_X, test_sub_y)\n",
    "\n",
    "# Save the model\n",
    "save_model(\"Gaussian Process Classifier\", grid_search)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gaussian Process Classification Analysis\n",
    "\n",
    "Put a summary of your model's performance in this spot!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Copy-Paste all the GPC stuff here! Don't run the entire notebook, or you will train each model. That will take ages...\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Group041-Sp22)",
   "language": "python",
   "name": "pycharm-b7952dfa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}